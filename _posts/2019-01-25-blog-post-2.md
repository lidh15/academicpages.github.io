---
title: 'Review on deep learning based EEG analysis'
date: 2019-01-26
permalink: /posts/2019/01/blog-post-2/
tags:
  - Reading Notes 
  - EEG
  - Deep Learning
---

Yesterday [a review](https://arxiv.org/abs/1901.05498) appeared in my daily information stream.
The author registered [http://dl-eeg.com](https://github.com/hubertjb/dl-eeg-review) for his GitHub repo, which make us believe this review should be great. 
Since I'm working on this for my bachelor thesis, I think it is necessary to read this review and ask myself why I cannot write such a review after reading hundreds of papers.
Traditionally, a reading notes should consist of my understanding rather than paragraphs of raw paper, but lots of illustrations and expressions in this work are just what I want to say. 

The topic of these studies is summarized as following:

![Fig. 4](/images/20190126Fig4.png 'Focus of the studies')

In the first page, the author says:
>More importantly, however, we noticed studies often suffer from poor reproducibility: a majority of papers would behard or impossible to reproduce given the unavailability of their data and code.

This mainly comes from three attributes of EEG: low signal-to-noise ratio (SNR), non-stationary signal, and high inter-subject variability.
Though it's believed that we have [lots of methods](https://hal.inria.fr/hal-01846433/document) dealing with these problems, none of them are really widely used.

To support a huge set of parameters, the amount of data is very important.
Every time I was asked to talk about my thesis, the first thing I would like to talk is the lack of data.
So far we have been working on data augmentation, Mr. Wenhao Xu is exploring the possibilities of using GAN to generate more EEG data.
In this paper, adding Guassian noise are mentioned to be helpful in several researches, which can be a good idea for us to try.
And overlapping windows was used in my work before, but I'm not considering it now because our hypothesis is related to certain time after stimuli rather than the state during a whole experiment.
And made use of the data that is usually thrown away when downsampling EEG in the preprocessing stage was also tried:
>It is common to downsample a signal acquired at higher sampling rate to 256 Hz or less. In their case, they reused the data thrown away during that step as new samples: a downsampling by a factor of N would therefore allow an augmentation of N times.

But I think these data generated via this method would have too much correlationship, and almost replicated.

In the preprocessing part, the author says:
>Given those results, we are encouraged to believe that using DNNs on EEG might be a way to avoid the explicit artifact removal step of the classical EEG processing pipeline without harming task performance.

That's also I'm encouraged to believe.

In the feature part, I have been trying PSD recently:
>Widely adopted by the EEG community, the power spectral density (PSD) of classical frequency bands from around 1 Hz to 40 Hz were used as features. Specifically, authors selected the delta (1-4 Hz), theta (5-8 Hz), alpha (9-13 Hz), lower beta (14-16 Hz), higher beta (17-30 Hz), and gamma (31-40 Hz) bands for mental workload state recognition. 

In the validation part, the following is what I want to highlight:
>In the case of inter-subject classification, the choice of the validation procedure can have a big impact on the reported performance of a model. The Leave-N-Subject-Out procedure, which uses different subjects for training and for testing, may lead to lower performance, but is applicable to real-life scenarios where a model must be used on a subject for whom no training data is available. In contrast, using k-fold cross-validation on the combined data from all the subjects often means that the same subjects are seen in both the training and testing sets. In the selected studies, 22 out of the 108 studies using an inter-subject approach used a Leave-N-Subjects-Out or Leave-One-Subjects-Out procedure.

So after reading the whole paper, I find a problem: the ideas in it are all known to me.
Is there anything new?
Fortunately, I find some model inspection techniques were used in a single study:
>[class activation map (CAM)](https://arxiv.org/pdf/1805.11704), [Deeplift and ablation of filters](https://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta), [saliency maps](https://arxiv.org/pdf/1710.00633), [input-feature unit-output correlation maps](https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.23730), [retrieval of closest examples](https://arxiv.org/pdf/1803.09702), [analysis of performance with transferred layers](https://link.springer.com/chapter/10.1007/978-3-319-58628-1_4), [analysis of most-activating input windows](https://arxiv.org/pdf/1711.07792), [analysis of generated outputs](https://arxiv.org/pdf/1806.01875).

The challenge I'm facing, in fact, is how to make sure if these possible methods are helpful to my work and make full use of all knowledge.
